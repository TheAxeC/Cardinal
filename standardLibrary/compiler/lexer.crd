
import "tokens" from Token, TokenType, TokenList

// Error class
class Error {
	field message

	// Set when triggered
	field isTriggered

	construct new() {
		isTriggered = false
	}

	// Set a message
	setMessage(str) {
		if (isTriggered) return
		isTriggered = true
		message = str
	}

	// Check the message
	checkMessage() {
		if (isTriggered) {
			isTriggered = false
			return message
		}
		return null
	}
}

class Lexer {

	// Represents the bytevalue of a
	static field a
	// Represents the bytevalue of a
	static field A 
	// Represents the bytevalue of a
	static field z
	// Represents the bytevalue of a
	static field Z
	// Represents the bytevalue of a
	static field nb0
	// Represents the bytevalue of a
	static field nb9

	// Returns true if [c] is a valid (non-initial) identifier character.
	static isName(char) {
		var elem = char.byteAt(0)
		return (elem >= a && elem <= z) || (elem >= A && elem <= Z) || char == "@" || char == "_"
	}

	// Returns true if [c] is a digit.
	static isDigit(char) {
		var elem = char.byteAt(0)
		return elem >= nb0 && elem <= nb9
	}

	// Read a name
	static readName(str) {
		var regex = Regex.new
		regex.compile("[a-z|_|@|A-Z][\\w|@]*")
		if (regex.match(str)) return str
		return null
	}

	// Read a number
	static readNumber(str) {
		var regex = Regex.new
		regex.compile("[0-9]+")
		if (!regex.match(str)) return null

		return Num.fromString(str)
	}

	// read a string
	static readString(str) {
		var ind = 1
		while (ind < str.count) {
			if (str.startsWith("\"") && str[ind] == "\"" && ind < str.count-1) return null

			if (str[ind] == "\\") {
				ind = ind + 1
			}
			ind = ind + 1
		}

		if (str.startsWith("\"")) return str
		return null
	}

	// read a comment
	static readBlockComment(str) {
		if (!str.startsWith("/*")) return null

		var nesting = 1
		var ind = 2
		while (nesting > 0 && ind < str.count) {
			if (str[ind] == "/") {
				if (ind < str.count - 1 && str[ind + 1] == "*") {
					nesting = nesting + 1
					ind = ind + 1
				}
			} else if (str[ind] == "*") {
				if (ind < str.count - 1 && str[ind + 1] == "/") {
					nesting = nesting - 1
					ind = ind + 1
				}
			}
			ind = ind + 1
		}

		if (nesting == 0 && ind < str.count) return null

		return str
	}

	// read a line comment
	static readlineComment(str) {
		var regex = Regex.new
		regex.compile("//.*\\n.")
		if (regex.match(str)) return null

		if (str.startsWith("//")) return str
		return null
	}

	// Check the token
	static checkString(str, notMax) {
		if (str == "") return TokenType::TOKEN_ERROR
		if (!notMax) return TokenType::TOKEN_EOF

		if (str == "\n") return TokenType::TOKEN_LINE

		var list = TokenList[str[0]]
		var ind = 1
		while (list != null && ind < str.count) {
			list = list[str[ind]]
			ind = ind + 1
		}

		// we have a token or null
		if (list != null && list[null] != null) return list[null]

		// we have a number, name or error
		if (readNumber(str) != null) return TokenType::TOKEN_NUMBER
		if (readName(str) != null) return TokenType::TOKEN_NAME

		// we could have a string
		if (readString(str) != null) return TokenType::TOKEN_STRING

		// We could have a line comment
		if (readlineComment(str) != null) return TokenType::TOKEN_COMMENT

		// Or a block comment
		if (readBlockComment(str) != null) return TokenType::TOKEN_COMMENT

		return null
	}

	static readError(str) {
		if (isDigit(str[0])) return "Number is invalid. "
		if (isName(str[0])) return "Invalid name."
		if (str[0] == "\"") return "Unterminated string."

		if (!str.startsWith("/*")) return "Invalid character -" + str[0] + "- "

		var nesting = 1
		var ind = 2
		while (nesting > 0 && ind < str.count) {
			if (str[ind] == "/") {
				if (ind < str.count - 1 && str[ind + 1] == "*") {
					nesting = nesting + 1
					ind = ind + 1
				}
			} else if (str[ind] == "*") {
				if (ind < str.count - 1 && str[ind + 1] == "/") {
					nesting = nesting - 1
					ind = ind + 1
				}
			}
			ind = ind + 1
		}

		if (nesting != 0) return "Unterminated block comment "

		return "error"
		
	}

	pre {
		a = "a".byteAt(0)
		A = "A".byteAt(0)
		z = "z".byteAt(0)
		Z = "Z".byteAt(0)
		nb0 = "0".byteAt(0)
		nb9 = "9".byteAt(0)
	}
}

// The parser responsible for translating the source into tokens
class Parser {
	// current module we are working in
	field _module
	
	// path to the source
	field _sourcePath
	
	// the source
	field _source
	
	// current place in the source
	field _ind
	
	// current token
	field _currentToken

	// index of the start of a token
	field _tokenStart
	
	// current line in the source
	field _currentLine
	
	// the previous token
	field _previousToken

	// indicates whether an error was found
	field _hasError

	construct new(source, sourcePath, mod) {
		_hasError = false
		_source = source
		_sourcePath = sourcePath
		_module = mod
		_ind = 0
		_currentToken = Token.new(null, "", 0)
		_currentLine = 1
		
	}

	nextToken() {
		_previousToken = _currentToken

		_tokenStart = _ind
		var prev = null
		var prevString = "\0"
		while (true) {
			var notMax = _ind < _source.count
			var str = "\0"
			if (notMax) str = _source[_tokenStart .. _ind]
			var token = Lexer.checkString(str, notMax)
			if ((token == null && prev != null) || (!notMax)) {
				if (prevString != "\0" && !notMax && prev == null) {
					if (!_hasError) lexError(Lexer.readError(prevString))
					_ind = _tokenStart + 1
					_hasError = true
					prev = Lexer.checkString("", false)
				} else {
					_hasError = false
				}
				if (prev == null) prev = Lexer.checkString(prevString, false)
				return makeToken(prevString, prev)
			}
			_ind = _ind + 1
			prev = token
			prevString = str
		}
	}

	// Sets the parser's current token to the given [type] and current character
	// range.
	makeToken(str, type) {
		_currentToken = Token.new(type, str, _currentLine)
		if (_currentToken.type == TokenType::TOKEN_LINE) _currentLine = _currentLine + 1
		return _currentToken
	}

	lexError(error) {
		_hasError = true
		IO.print("    error: ")
		IO.print("[", _sourcePath ," line ", _currentLine, "] Error: ")
		IO.println(error)
	}

	warning(warning) {
		var token = _previousToken
		if (token.content == null) return
		IO.print("  warning: ")
		IO.print("[", _sourcePath ," line ", _currentLine, "] Warning: ")
		IO.print(token.content)
		IO.println(warning)
	}

	error(err, token) {
		var token = _previousToken
		if (token.content == null) return
		IO.print("  error: ")
		IO.print("[", _sourcePath ," line ", token.line, "] Error: ")
		IO.print(token.content)
		IO.println(err)
	}

}


// Tokenizes the source code
class Tokenizer {

	// list of all tokens
	field _tokens

	// current index in the token list
	field _ind

	// field to store current positions on a certain name
	field _map

	// A list with all types to ignore
	field _ignoreTypes

	// The lexer used
	field _lexer

	construct new(lexer) {
		_tokens = []
		_ind = 0
		_map = {}
		_lexer = lexer
		_ignoreTypes = []

		// Ignore these types
		_ignoreTypes.add(TokenType::TOKEN_COMMENT)
		_ignoreTypes.add(TokenType::TOKEN_SPACE)
		_ignoreTypes.add(TokenType::TOKEN_CARRIAGE)
	}

	// Check if we want to ignore a type
	contains(type) {
		for(item in _ignoreTypes) {
			if (type == item) return false
		}
		return true
	}

	// Tokenizes a file
	tokenize() {
		var tok = _lexer.nextToken()
		while (tok.type != TokenType::TOKEN_EOF) {
			if (contains(tok.type)) _tokens.add(tok)
			tok = _lexer.nextToken()
		}
		_tokens.add(tok)
	}

	// Save to token to come to later
	saveToken(name) {
		_map[name] = _ind
	}

	// Load a token from the map
	loadToken(name) {
		var cur = _map[name]
		if (cur != null) _ind = cur
	}

	peek() { _tokens[_ind] }

	next() {
		if (_ind < _tokens.count)
			_ind = _ind + 1
		return _tokens[ind - 1]
	}

	prev() { _tokens[_ind - 1] } 

	consume(err) {
		var next = next()
		if (next.type != expected) {
			_lexer.error(err, next)
		}
		return next
	}
}

