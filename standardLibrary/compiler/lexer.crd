
import "tokens" from Token, TokenType

class Lexer {

	// Represents the bytevalue of a
	static field a
	// Represents the bytevalue of a
	static field A 
	// Represents the bytevalue of a
	static field z
	// Represents the bytevalue of a
	static field Z
	// Represents the bytevalue of a
	static field nb0
	// Represents the bytevalue of a
	static field nb9

	// Returns true if [c] is a valid (non-initial) identifier character.
	static isName(char) {
		var elem = char.byteAt(0)
		return (elem >= a && elem <= z) || (elem >= A && elem <= Z) || char == "@" || char == "_"
	}

	// Returns true if [c] is a digit.
	static isDigit(char) {
		var elem = char.byteAt(0)
		return elem >= nb0 && elem <= nb9
	}

	pre {
		a = "a".byteAt(0)
		A = "A".byteAt(0)
		z = "z".byteAt(0)
		Z = "Z".byteAt(0)
		nb0 = "0".byteAt(0)
		nb9 = "9".byteAt(0)
	}
}

// The parser responsible for translating the source into tokens
class Parser {
	// current module we are working in
	field _module
	
	// path to the source
	field _sourcePath
	
	// the source
	field _source
	
	// current place in the source
	field _ind
	
	// current token
	field _currentToken

	// index of the start of a token
	field _tokenStart
	
	// current line in the source
	field _currentLine
	
	// the previous token
	field _previousToken

	// indicates whether an error was found
	field _hasError

	// Returns the current character the parser is sitting on.
	peekChar() { _source[ind] }

	// Returns the character after the current character.
	peekNextChar() {
		if (peekChar() == "\0") return "\0"
		return _source[ind + 1]
	}

	// Advances the parser forward one character.
	nextChar() {
		var c = peekChar()
		_ind = _ind + 1
		if (c == "\n") _currentLine = _currentLine + 1
		return c
	}

	// Sets the parser's current token to the given [type] and current character
	// range.
	makeToken(type) {
		_currentToken = Token.new(type, _source[_tokenStart .. _ind], _currentLine)
		if (type == TokenType::TOKEN_LINE) _currentLine = _currentLine - 1
	}

	// If the current character is [c], then consumes it and makes a token of type
	// [two]. Otherwise makes a token of type [one].
	twoCharToken(char, type2, type1) {
		if (peekChar() == char) {
			nextChar()
			makeToken(type2)
			return
		}
		makeToken(type1)
	}

	// Skips the rest of the current line.
	// Comment is started by a //
	skipLineComment() {
		while (peekChar() != "\n" && peekChar() != "\0") nextChar()
	}

	// Skips the rest of a block comment.
	// Comments are opened by /* and closed by */
	skipBlockComment() {
		nextChar()

		var nesting = 1
		while (nesting > 0) {
			if (peekChar() == "\0") {
				lexError("Unterminated block comment.")
				return
			}

			if (peekChar() == "/" && peekNextChar() == "*") {
				nextChar()
				nextChar()
				nesting = nesting + 1
			} else if (peekChar() == "*" && peekNextChar() == "/") {
				nextChar()
				nextChar()
				nesting = nesting - 1
			} else { nextChar() }
		}
	}

	

	lexError(error) {
		_hasError = true
		IO.print("    error: ", args)
		IO.print("[", _sourcePath ," line ", _currentLine, "] Error: ")
		IO.println(error)
	}

	warning(warning) {
		var token = _previousToken

		if (token.type == TokenType::TOKEN_ERROR) return

		IO.print("  warning: ")
		IO.print("[", _sourcePath ," line ", _currentLine, "] Warning: ")
		if (token.type == TokenType::TOKEN_LINE) { 
			IO.print("newline: ") 
		} else if (token.type == TokenType::TOKEN_EOF) {
			IO.print("end of file: ")
		} else IO.print(token.str)
		IO.println(warning)
	}

	error(error) {
		var token = _previousToken

		if (token.type == TokenType::TOKEN_ERROR) return

		IO.print("  error: ")
		IO.print("[", _sourcePath ," line ", _currentLine, "] Error: ")
		if (token.type == TokenType::TOKEN_LINE) { 
			IO.print("newline: ") 
		} else if (token.type == TokenType::TOKEN_EOF) {
			IO.print("end of file: ")
		} else IO.print(token.str)
		IO.println(error)
	}

}

