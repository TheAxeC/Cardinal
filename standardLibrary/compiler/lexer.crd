
import "tokens" from Token, TokenType, TokenList

class Lexer {

	// Represents the bytevalue of a
	static field a
	// Represents the bytevalue of a
	static field A 
	// Represents the bytevalue of a
	static field z
	// Represents the bytevalue of a
	static field Z
	// Represents the bytevalue of a
	static field nb0
	// Represents the bytevalue of a
	static field nb9

	// Returns true if [c] is a valid (non-initial) identifier character.
	static isName(char) {
		var elem = char.byteAt(0)
		return (elem >= a && elem <= z) || (elem >= A && elem <= Z) || char == "@" || char == "_"
	}

	// Returns true if [c] is a digit.
	static isDigit(char) {
		var elem = char.byteAt(0)
		return elem >= nb0 && elem <= nb9
	}

	// Read a name
	static readName(str) {
		if (!Lexer.isName(str[0])) return null
		if (str.contains(" ")) return null
		return str[0...(str.count)]
	}

	// Read a number
	static readNumber(str) {
		return Num.fromString(str[0...(str.count)])
	}

	// Check the token
	static checkString(str, notMax) {
		if (!notMax) return TokenType::TOKEN_EOF

		var list = TokenList[str[0]]
		var ind = 1
		while (list != null && ind < str.count) {
			list = list[str[ind]]
			ind = ind + 1
		}

		// we have a token or null
		if (list != null && list[null] != null) return list[null]

		// we have a number, name or error
		if (readNumber(str) != null) return TokenType::TOKEN_NUMBER
		if (readName(str) != null) return TokenType::TOKEN_NAME
		return null
	}

	pre {
		a = "a".byteAt(0)
		A = "A".byteAt(0)
		z = "z".byteAt(0)
		Z = "Z".byteAt(0)
		nb0 = "0".byteAt(0)
		nb9 = "9".byteAt(0)
	}
}

// The parser responsible for translating the source into tokens
class Parser {
	// current module we are working in
	field _module
	
	// path to the source
	field _sourcePath
	
	// the source
	field _source
	
	// current place in the source
	field _ind
	
	// current token
	field _currentToken

	// index of the start of a token
	field _tokenStart
	
	// current line in the source
	field _currentLine
	
	// the previous token
	field _previousToken

	// indicates whether an error was found
	field _hasError

	construct new(source, sourcePath, mod) {
		_hasError = false
		_source = source
		_sourcePath = sourcePath
		_module = mod
		_ind = 0
		_currentToken = Token.new(null, "", 0)
		_currentLine = 0
	}

	nextToken() {
		_previousToken = _currentToken

		_tokenStart = _ind
		var prev = null
		var prevString = "\0"
		while (true) {
			var notMax = _ind < _source.count
			var str = "\0"
			if (notMax) str = _source[_tokenStart .. _ind]
			var token = Lexer.checkString(str, notMax)
			if ((token == null && prev != null) || (prev == token && !notMax)) return makeToken(prevString, prev)
			_ind = _ind + 1
			prev = token
			prevString = str
		}
	}

	// Sets the parser's current token to the given [type] and current character
	// range.
	makeToken(str, type) {
		_currentToken = Token.new(type, str, _currentLine)
		if (_currentToken.str == "\n") _currentLine = _currentLine - 1
		return _currentToken
	}

	lexError(error) {
		_hasError = true
		IO.print("    error: ", args)
		IO.print("[", _sourcePath ," line ", _currentLine, "] Error: ")
		IO.println(error)
	}

	warning(warning) {
		var token = _previousToken
		if (token.content == null) return
		IO.print("  warning: ")
		IO.print("[", _sourcePath ," line ", _currentLine, "] Warning: ")
		IO.print(token.content)
		IO.println(warning)
	}

	error(error) {
		var token = _previousToken
		if (token.content == null) return
		IO.print("  error: ")
		IO.print("[", _sourcePath ," line ", _currentLine, "] Error: ")
		IO.print(token.content)
		IO.println(error)
	}

}

