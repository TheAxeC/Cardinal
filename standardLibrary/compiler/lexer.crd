
import "tokens" from Token, TokenType, TokenList

// Error class
class Error {
	field message

	// Set when triggered
	field isTriggered

	construct new() {
		isTriggered = false
	}

	// Set a message
	setMessage(str) {
		if (isTriggered) return
		isTriggered = true
		message = str
	}

	// Check the message
	checkMessage() {
		if (isTriggered) {
			isTriggered = false
			return message
		}
		return null
	}
}
class Lexer {

	// Represents the bytevalue of a
	static field a
	// Represents the bytevalue of a
	static field A 
	// Represents the bytevalue of a
	static field z
	// Represents the bytevalue of a
	static field Z
	// Represents the bytevalue of a
	static field nb0
	// Represents the bytevalue of a
	static field nb9

	// Returns true if [c] is a valid (non-initial) identifier character.
	static isName(char) {
		var elem = char.byteAt(0)
		return (elem >= a && elem <= z) || (elem >= A && elem <= Z) || char == "@" || char == "_"
	}

	// Returns true if [c] is a digit.
	static isDigit(char) {
		var elem = char.byteAt(0)
		return elem >= nb0 && elem <= nb9
	}

	// Read a name
	static readName(str) {
		var regex = Regex.new
		regex.compile("[a-z|_|@|A-Z][\\w|@]*")
		if (regex.match(str)) return str
		return null
	}

	// Read a number
	static readNumber(str) {
		var regex = Regex.new
		regex.compile("[0-9]+")
		if (!regex.match(str)) return null

		return Num.fromString(str)
	}

	// read a string
	static readString(str) {
		var ind = 1
		while (ind < str.count) {
			if (str.startsWith("\"") && str[ind] == "\"" && ind < str.count-1) return null

			if (str[ind] == "\\") {
				ind = ind + 1
			}
			ind = ind + 1
		}

		if (str.startsWith("\"")) return str
		return null
	}

	// read a comment
	static readBlockComment(str) {
		if (!str.startsWith("/*")) return null

		var nesting = 1
		var ind = 2
		while (nesting > 0 && ind < str.count) {
			if (str[ind] == "/") {
				if (ind < str.count - 1 && str[ind + 1] == "*") {
					nesting = nesting + 1
					ind = ind + 1
				}
			} else if (str[ind] == "*") {
				if (ind < str.count - 1 && str[ind + 1] == "/") {
					nesting = nesting - 1
					ind = ind + 1
				}
			}
			ind = ind + 1
		}

		if (nesting == 0 && ind < str.count) return null

		return str
	}

	// read a line comment
	static readlineComment(str) {
		var regex = Regex.new
		regex.compile("//.*\\n.")
		if (regex.match(str)) return null

		if (str.startsWith("//")) return str
		return null
	}

	// Check the token
	static checkString(str, notMax) {
		if (!notMax) return TokenType::TOKEN_EOF

		if (str == "\n") return TokenType::TOKEN_LINE

		var list = TokenList[str[0]]
		var ind = 1
		while (list != null && ind < str.count) {
			list = list[str[ind]]
			ind = ind + 1
		}

		// we have a token or null
		if (list != null && list[null] != null) return list[null]

		// we have a number, name or error
		if (readNumber(str) != null) return TokenType::TOKEN_NUMBER
		if (readName(str) != null) return TokenType::TOKEN_NAME

		// we could have a string
		if (readString(str) != null) return TokenType::TOKEN_STRING

		// We could have a line comment
		if (readlineComment(str) != null) return TokenType::TOKEN_COMMENT

		// Or a block comment
		if (readBlockComment(str) != null) return TokenType::TOKEN_COMMENT

		return null
	}

	pre {
		a = "a".byteAt(0)
		A = "A".byteAt(0)
		z = "z".byteAt(0)
		Z = "Z".byteAt(0)
		nb0 = "0".byteAt(0)
		nb9 = "9".byteAt(0)
	}
}

// The parser responsible for translating the source into tokens
class Parser {
	// current module we are working in
	field _module
	
	// path to the source
	field _sourcePath
	
	// the source
	field _source
	
	// current place in the source
	field _ind
	
	// current token
	field _currentToken

	// index of the start of a token
	field _tokenStart
	
	// current line in the source
	field _currentLine
	
	// the previous token
	field _previousToken

	// indicates whether an error was found
	field _hasError

	// A list with all types to ignore
	field _ignoreTypes

	construct new(source, sourcePath, mod) {
		_hasError = false
		_source = source
		_sourcePath = sourcePath
		_module = mod
		_ind = 0
		_currentToken = Token.new(null, "", 0)
		_currentLine = 0
		_ignoreTypes = []

		// Ignore these types
		_ignoreTypes.add(TokenType::TOKEN_COMMENT)
		_ignoreTypes.add(TokenType::TOKEN_SPACE)
		_ignoreTypes.add(TokenType::TOKEN_CARRIAGE)
	}

	nextToken() {
		_previousToken = _currentToken

		_tokenStart = _ind
		var prev = Lexer.checkString("", false)
		var prevString = "\0"
		while (true) {
			var notMax = _ind < _source.count
			var str = "\0"
			if (notMax) str = _source[_tokenStart .. _ind]
			var token = Lexer.checkString(str, notMax)
			if ((token == null && prev != null) || (!notMax)) {
				if (prevString != "\0" && !notMax) lexError("error")
				return makeToken(prevString, prev)
			}
			_ind = _ind + 1
			prev = token
			prevString = str
		}
	}

	// Sets the parser's current token to the given [type] and current character
	// range.
	makeToken(str, type) {
		_currentToken = Token.new(type, str, _currentLine)
		if (_currentToken.type == TokenType::TOKEN_LINE) _currentLine = _currentLine + 1
		return _currentToken
	}

	lexError(error) {
		_hasError = true
		IO.print("    error: ")
		IO.print("[", _sourcePath ," line ", _currentLine, "] Error: ")
		IO.println(error)
	}

	warning(warning) {
		var token = _previousToken
		if (token.content == null) return
		IO.print("  warning: ")
		IO.print("[", _sourcePath ," line ", _currentLine, "] Warning: ")
		IO.print(token.content)
		IO.println(warning)
	}

	error(err) {
		var token = _previousToken
		if (token.content == null) return
		IO.print("  error: ")
		IO.print("[", _sourcePath ," line ", _currentLine, "] Error: ")
		IO.print(token.content)
		IO.println(err)
	}

	// Peek at the next type
	peek() { _currentToken.type }

	// Consumes the current token if its type is [expected]. Returns true if a
	// token was consumed.
	match(expected) {
		if (peek() != expected) return false
		nextToken()
		return true
	}

	// Matches one or more newlines. Returns true if at least one was found.
	matchLine() {
		if (!match(TokenType::TOKEN_LINE)) return false
		while (match(TokenType::TOKEN_LINE)) {}
		return true
	}

	// Consumes the current token. Emits an error if its type is not [expected].
	// Returns the consumed token.
	consume(expected, err) {
		nextToken()
		if (_previousToken.type != expected) {
			error(err)

			// If the next token is the one we want, assume the current one is just a
			// spurious error and discard it to minimize the number of cascaded errors.
			if (_currentToken.type == expected) nextToken()
		}
	}
}

